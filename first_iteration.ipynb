{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necassary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "import re\n",
    "#import inflect\n",
    "#from num2words import num2words\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necassary preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df = pd.read_csv('us_equities_news_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out Nvidia articles\n",
    "nvidia_df = df[df['title'].str.contains('NVIDIA|NVDA', case=False, na=False) |\n",
    "               df['content'].str.contains('NVIDIA|NVDA', case=False, na=False) |\n",
    "               df['ticker'].str.contains('NVIDIA|NVDA', case=False, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(nvidia_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre processing steps\n",
    "def preprocess_text_iteration1(text):\n",
    "    text = text.strip()\n",
    "    # Replace URLs with an empty string\n",
    "    re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    words = word_tokenize(text.lower())\n",
    "    #check if the word is alpha numeric\n",
    "    alpha_words = [word for word in words if word.isalpha()]\n",
    "    #remove numbers\n",
    "    numbers_words = [word for word in alpha_words if not word.isdigit()]\n",
    "    return numbers_words\n",
    "\n",
    "# apply preprocessing\n",
    "nvidia_df['content'] = nvidia_df['content'].apply(preprocess_text_iteration1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a document term matrix\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(nvidia_df['content'].apply(lambda x: ' '.join(x)))\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train LDAModel\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=0)\n",
    "lda.fit(X)\n",
    "\n",
    "# get topics\n",
    "def get_topics(model, vectorizer, n_top_words):\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    topics = []\n",
    "    for topic in model.components_:\n",
    "        topic_words = [words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        topics.append(topic_words)\n",
    "    return topics\n",
    "\n",
    "topics = get_topics(lda, vectorizer, 10)\n",
    "print(topics)\n",
    "for i, topic in enumerate(topics):\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic)))\n",
    "\n",
    "\n",
    "# get topic distribution for each document\n",
    "topic_dist = lda.transform(X)\n",
    "print(topic_dist)\n",
    "\n",
    "# plot the distribution of topics\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(topic_dist.argmax(axis=1), bins=5)\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Number of documents')\n",
    "plt.title('Distribution of topics')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FuzzyTM FLSA-W\n",
    "from FuzzyTM import FLSA_W\n",
    "\n",
    "# Coherence and Diversity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a cluster plot for the topics\n",
    "\n",
    "#if we have more time we look this because it is a cool plot (we don't fully understand it because copilot made it)\n",
    "\n",
    "# from sklearn.manifold import TSNE\n",
    "# tsne = TSNE(n_components=2, random_state=0)\n",
    "# tsne_embedding = tsne.fit_transform(topic_dist)\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# sns.scatterplot(x=tsne_embedding[:, 0], y=tsne_embedding[:, 1], hue=topic_dist.argmax(axis=1), palette='tab10')\n",
    "# plt.xlabel('t-SNE component 1')\n",
    "# plt.ylabel('t-SNE component 2')\n",
    "# plt.title('t-SNE plot of topics')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLSA-W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# create a FLSA-W model\n",
    "flsaW = FLSA_W(\n",
    "input_file = nvidia_df['content'].to_list(),\n",
    "num_topics=5,\n",
    "num_words=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flsaW.get_vocabulary_size()\n",
    "pwgt, ptgd = flsaW.get_matrices() # THIS TRAINS THE MODEL\n",
    "print(flsaW.show_topics())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in flsaW.show_topics(representation='words'):\n",
    "    print(topic)\n",
    "\n",
    "\n",
    "print(flsaW.get_coherence_score()) # ask if this has to be high\n",
    "print(flsaW.get_diversity_score()) # ask if this has to be high\n",
    "print(flsaW.get_interpretability_score()) # ask if this has to be high\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coherence for lda\n",
    "print(\n",
    "flsaW.get_coherence_score(\n",
    "nvidia_df['content'].to_list(),\n",
    "topics)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diversity for lda\n",
    "print(flsaW.get_diversity_score(topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elbow plot for FLSA-W\n",
    "range_n_topics = range(1, 11)\n",
    "\n",
    "# Create a list of coherence values\n",
    "coherences = []\n",
    "for n_topics in range_n_topics:\n",
    "    print(n_topics)\n",
    "    flsaW_elbow = FLSA_W(\n",
    "    input_file = nvidia_df['content'].to_list(),\n",
    "    num_topics=n_topics,\n",
    "    num_words=10,\n",
    "    )\n",
    "    flsaW_elbow.get_vocabulary_size()\n",
    "    pwgt, ptgd = flsaW_elbow.get_matrices() # THIS TRAINS THE MODEL\n",
    "    coherence = flsaW_elbow.get_coherence_score()\n",
    "    coherences.append(coherence)\n",
    "\n",
    "# Plot the elbow plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range_n_topics, coherences, 'bx-')\n",
    "plt.xlabel('Number of topics')\n",
    "plt.ylabel('Coherence')\n",
    "plt.title('Elbow Method For Optimal Number of Topics')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elbow plot for diversity\n",
    "\n",
    "#elbow plot for FLSA-W\n",
    "range_n_topics = range(1, 11)\n",
    "\n",
    "# Create a list of coherence values\n",
    "diversities = []\n",
    "for n_topics in range_n_topics:\n",
    "    print(n_topics)\n",
    "    flsaW_elbow = FLSA_W(\n",
    "    input_file = nvidia_df['content'].to_list(),\n",
    "    num_topics=n_topics,\n",
    "    num_words=10,\n",
    "    )\n",
    "    flsaW_elbow.get_vocabulary_size()\n",
    "    pwgt, ptgd = flsaW_elbow.get_matrices() # THIS TRAINS THE MODEL\n",
    "    diversity = flsaW_elbow.get_diversity_score()\n",
    "    coherences.append(diversity)\n",
    "\n",
    "# Plot the elbow plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range_n_topics, coherences, 'bx-')\n",
    "plt.xlabel('Number of topics')\n",
    "plt.ylabel('Diversity')\n",
    "plt.title('Elbow Method For Optimal Number of Topics')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "docs = [' '.join(doc) for doc in nvidia_df['content']]\n",
    "# Create the model (uses DistilBERT by default)\n",
    "bert_topic = BERTopic()\n",
    "# Train the model and transform your data into topics\n",
    "topic_assigned_to_doc, _ = bert_topic.fit_transform(docs)\n",
    "topic_matrix = bert_topic.get_topic_info()\n",
    "bert_topic.get_topics()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topic_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_topic.visualize_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_matrix.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the name of the topic matrix and the representative docs using for loop\n",
    "for i in range(0, 11):\n",
    "    print(topic_matrix['Name'][i])\n",
    "    print(topic_matrix['Representative_Docs'][i])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of the first 10 representative docs\n",
    "rep_docs = topic_matrix['Representation'][1:6].to_list()\n",
    "print(rep_docs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #bert coherence --> just say we tried, in the end it doesn't matter I tried hard and got so far \n",
    "# print(\n",
    "# flsaW.get_coherence_score(\n",
    "# nvidia_df['content'].to_list(),\n",
    "# rep_docs)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert diversity\n",
    "print(flsaW.get_diversity_score(rep_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "topic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
