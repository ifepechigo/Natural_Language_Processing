Defaults for models: unless specified otherwise
5 topics 
10 words

**Iteration 0:
- lowercase
- remove url
- remove numbers
- alpha words

Conclusions: 
Coherence matrix for Bertopic model didnt work / took too long to run. We assume this is because not enough preprocessing was done. 
This resulted in too many words that needed to be compared against each other.

 Topic: 
 Topic with the most amount of documents contained mostly stopwords 
 The topic with the least amount of doucments actually contained specific words such as nasdaq

LDA
diversity scores: 0.42
coherence: 0.32
interpretability: 0.13

FLSA-W:
diversity scores: 0.9
coherence: 0.41
interpretability: 0.37


**Iteration 1:
- remove stopwords
- Adjust number of topics 
- adjust number of words per topic
# I decided to move these here since we can already make a decision based on the prev iteation.

Bertopic 
- Use default DistilBERT

diversity scores: 0.9
coherence: 0.69
interpretability: 0.622

FLSA-W:
diversity scores: 0.98
coherence: 0.46
interpretability: 0.456

LDA
diversity scores: 0.68
coherence: 0.48
interpretability: 0.32

Bertopic 
- Use default DistilBERT

diversity scores: 0.96
coherence: 0.68
interpretability: 0.37


conclusion: 
removing stopwords clearly changed our distribution. Meaning the removal worked properly

Iteration 2:
- add lemamtization
- add titles 
- limit features in vector matrix # I don't know what htis means (or don't remember)
#if it is for LDA i don't know if we should do it because then we are changing
#the data so coherence and diversity would not be computable.

Iteration 3:
- hyper parameter tuning

-------------------------------------------------------------------------------------------------------------------------------------------

Questions to ask: 

* Should it be fully optimized or somewhat optimized? 
* How many iterations 2 or 3?
* 



-------------------------------------------------------------------------------------------------------------------------------------------

Justifications: 

* Justify use of t-SNE
-- Look into t-SNE distribution for topic visualization
-- Look into how the clusters reform by adjusting number of clusters

* Bertopic is already pretrtained, so we decided to train our own word2vec model for better comparison.

* Skipgram vs. CBOW
 - Skipgram was chosen because the meaning of words is more meaningful for the topic than the grammar
 - dataset size is relatively small
 - skipgram is less prone to overfitting



